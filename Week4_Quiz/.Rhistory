}
makeCacheMatrix <- function(x = matrix()) {
i <- NULL
set <- function(y) {
x <<- y
i <<- NULL
}
get <- function() x
setinv <- function(inv) i <<- inv
getinv <- function() i
list(set = set,
get = get,
setinv = setinv,
getinv = getinv)
}
cacheSolve <- function(x, ...) {
i <- x$getinv()
if(!is.null(i)) {
message("getting cached data")
return(i)
}
data <- x$get()
i <- solve(data, ...)
x$setinv(i)
i
}
## Test
# Create sample matrix
x <- matrix( c(2, 4, 3, 1, 5, 6, 7, 3, 1), # the data elements
nrow=3,              # number of rows
ncol=3,              # number of columns
byrow = TRUE)
cacheSolve <- function(x, ...) {
i <- x$getinv()
if(!is.null(i)) {
message("getting cached data")
return(i)
}
data <- x$get()
i <- solve(data, ...)
x$setinv(i)
i
}
cacheSolve <- function(x, ...) {
i <- x$getinv()
if(!is.null(i)) {
message("getting cached data")
return(i)
}
data <- x$get()
i <- solve(data, ...)
x$setinv(i)
i
}
data(iris)
data(iris)
## Put comments here that give an overall description of what your
## functions do
## Write a short comment describing this function
# This function is similar to the makeVector function in the description
# Overall steps: set+get vector, then set+get inverse
# The goal is to make a special 'matrix'-object that can cache its inverse
makeCacheMatrix <- function(x = matrix()) {
i <- NULL
set <- function(y) {
x <<- y
i <<- NULL
}
get <- function() x
setinv <- function(inv) i <<- inv
getinv <- function() i
list(set = set,
get = get,
setinv = setinv,
getinv = getinv)
}
## Write a short comment describing this function
# The goal is to write a function that computes the inverse of matrix-object
# Approach is similar to the example in the assignment
# Only difference is that we use the solve-function and apply it (row39)
cacheSolve <- function(x, ...) {
i <- x$getinv()
if(!is.null(i)) {
message("getting cached data")
return(i)
}
data <- x$get()
i <- solve(data, ...)
x$setinv(i)
i
}
## Test
# Create sample matrix
x <- matrix( c(2, 4, 3, 1, 5, 6, 7, 3, 1), # the data elements
nrow=3,              # number of rows
ncol=3,              # number of columns
byrow = TRUE)
# Check for determinant and apply solve to get a hint of what we want
det(x)
solve(x)
# Now apply cacheSolve and see of the solution matches
x1 <- makeCacheMatrix(x)
cacheSolve(x1)
x <- matrix( c(2, 4, 3, 1, 5, 6, 7, 3, 1), # the data elements
nrow=3,              # number of rows
ncol=3,              # number of columns
byrow = TRUE)
###
# install.packages("swirl")
library(swirl)
swirl()
## Q1 - Download the following file
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv
fileUrlQ1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrlQ1, destfile = "q1.csv", method = "curl")
df_q1 <- read.csv("q1.csv")
View(df_q1)
View(df_q1)
str(df_q1$VAL)
length(which(df_q1$VAL == 24)) # 24 is code for properties > 1million
View(df_q1)
View(df_q1)
## Q2 - Consider the variable FES in the code book.
# Which of the "tidy data" principles does this variable violate?
unique(df_q1$FES)
str(df_q1$FES)
download.file(fileUrlQ3, destfile = "q3", method = "curl")
## Q3 - Download the Excel spreadsheet on Natural Gas Aquisition Program here:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx
fileUrlQ3 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrlQ3, destfile = "q3", method = "curl")
download.file(fileUrlQ3, destfile = "q3.xlsx", method = "curl")
install.packages('xlsx')
library(xlsx)
install.packages('readxl')
library(readxl)
df_q3 <- read_excel("q3.xlsx")
View(df_q3)
?readxl
?read_excel
dat <- read_excel("q3.xlsx", range = cell_rows(18:23), range = cell_cols(7:15))
dat <- read_excel("q3.xlsx", range = cell_rows(18:23))
View(dat)
dat <- dat[,c(7:15)]
View(dat)
sum(dat$Zip*dat$Ext,na.rm=T)
## Q4 - Read the XML data on Baltimore restaurants from here:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml
install.packages('XML')
## Q4 - Read the XML data on Baltimore restaurants from here:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml
library(XML)
fileUrlQ4 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrlQ4, useInternal = True)
fileUrlQ4 <- "https://https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrlQ4, useInternal = True)
doc <- xmlTreeParse(fileUrlQ4)
fileUrlQ4 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrlQ4)
fileUrlQ4 <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrlQ4)
doc <- xmlTreeParse(fileUrlQ4, useInternalNodes = TRUE)
## Q4 - Read the XML data on Baltimore restaurants from here:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml
library(XML)
fileUrlQ4 <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrlQ4, useInternalNodes = TRUE)
rootNode <- xmlRoot(doc)
names(rootNode)
rootNode[[1]]
rootNode[[1]][[2]]
rootNode[[1]][[1]]
rootNode[[1]][[1]][[3]]
rootNode[[1]][[1]][[2]]
xmlSApply(rootNode, 21231)
xmlSApply(rootNode, "21231")
sum(xpathSApply(rootnode,"//zipcode",xmlValue) == 21231) # number with this zip
sum(xpathSApply(rootNode,"//zipcode",xmlValue) == 21231) # number with this zip
## Q5 - Download the 2006 microdata survey about housing
# for the state of Idaho using download.file() from here:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv
# using the fread() command load the data into an R object
fileUrlQ5 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrlQ5, destfile = "q5.csv", method = "curl")
DT <-
?fread
DT <-
?fread()
DT = fread('Q5.csv')
install.packages('data.table')
library(data.table)
DT = fread('Q5.csv')
View(DT)
# DT[,mean(pwgtp15),by=SEX]
start_time <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
end_time <- Sys.time() # Answer 1
# mean(DT$pwgtp15,by=DT$SEX)
start_time <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX)
end_time <- Sys.time() # Answer 2
# sapply(split(DT$pwgtp15,DT$SEX),mean)
start_time <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
end_time <- Sys.time() # Answer 3
# tapply(DT$pwgtp15,DT$SEX,mean)
start_time <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
end_time <- Sys.time() # Answer 4
# mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
start_time <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
end_time <- Sys.time() # Answer 5
# rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
start_time <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
end_time <- Sys.time() # Answer 6
# DT[,mean(pwgtp15),by=SEX]
start_time <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
end_time <- Sys.time()
end_time - start_time # Answer 1
# mean(DT$pwgtp15,by=DT$SEX)
start_time <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX)
end_time <- Sys.time()
end_time - start_time # Answer 2
# sapply(split(DT$pwgtp15,DT$SEX),mean)
start_time <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
end_time <- Sys.time()
end_time - start_time # Answer 3
# tapply(DT$pwgtp15,DT$SEX,mean)
start_time <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
end_time <- Sys.time()
end_time - start_time # Answer 4
# mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
start_time <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
end_time <- Sys.time()
end_time - start_time # Answer 5
# rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
start_time <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
end_time <- Sys.time()
end_time - start_time # Answer 6
# rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
str(DT$SEX)
# rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
DT$SEX <- as.numeric(DT$SEX)
start_time <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
end_time <- Sys.time()
end_time - start_time # Answer 6
DT$SEX <- as.numeric(DT$SEX)
# DT[,mean(pwgtp15),by=SEX]
start_time <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
end_time <- Sys.time()
end_time - start_time # Answer 1
# mean(DT$pwgtp15,by=DT$SEX)
start_time <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX)
end_time <- Sys.time()
end_time - start_time # Answer 2
# sapply(split(DT$pwgtp15,DT$SEX),mean)
start_time <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
end_time <- Sys.time()
end_time - start_time # Answer 3
# tapply(DT$pwgtp15,DT$SEX,mean)
start_time <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
end_time <- Sys.time()
end_time - start_time # Answer 4
# mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
start_time <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
end_time <- Sys.time()
end_time - start_time # Answer 5
# rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
start_time <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
end_time <- Sys.time()
end_time - start_time # Answer 6
## Q3 - Download the Excel spreadsheet on Natural Gas Aquisition Program here:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx
fileUrlQ3 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrlQ3, destfile = "q3.xlsx", method = "curl")
library(readxl)
dat <- read_excel("q3.xlsx", range = cell_rows(18:23))
dat <- dat[,c(7:15)]
## Q5 - Download the 2006 microdata survey about housing
# for the state of Idaho using download.file() from here:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv
# using the fread() command load the data into an R object
fileUrlQ5 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrlQ5, destfile = "q5.csv", method = "curl")
library(data.table)
DT = fread('Q5.csv')
# DT[,mean(pwgtp15),by=SEX]
start_time <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
end_time <- Sys.time()
end_time - start_time # Answer 1
# mean(DT$pwgtp15,by=DT$SEX)
start_time <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX)
end_time <- Sys.time()
end_time - start_time # Answer 2
# sapply(split(DT$pwgtp15,DT$SEX),mean)
start_time <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
end_time <- Sys.time()
end_time - start_time # Answer 3
# tapply(DT$pwgtp15,DT$SEX,mean)
start_time <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
end_time <- Sys.time()
end_time - start_time # Answer 4
# mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
start_time <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
end_time <- Sys.time()
end_time - start_time # Answer 5
# rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
start_time <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
end_time <- Sys.time()
end_time - start_time # Answer 6
system.time(DT[,mean(pwgtp15),by=SEX])
# tapply(DT$pwgtp15,DT$SEX,mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
# mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
# mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
# tapply(DT$pwgtp15,DT$SEX,mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
# DT[,mean(pwgtp15),by=SEX]
system.time(DT[,mean(pwgtp15),by=SEX])
# mean(DT$pwgtp15,by=DT$SEX)
system.time(mean(DT$pwgtp15,by=DT$SEX))
# sapply(split(DT$pwgtp15,DT$SEX),mean)
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
# tapply(DT$pwgtp15,DT$SEX,mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
# Option 1: mean(DT$pwgtp15,by=DT$SEX)
system.time(mean(DT$pwgtp15,by=DT$SEX))
# Option 2: DT[,mean(pwgtp15),by=SEX]
system.time(DT[,mean(pwgtp15),by=SEX])
# Option 1: mean(DT$pwgtp15,by=DT$SEX)
system.time(mean(DT$pwgtp15,by=DT$SEX))
# Option 3: rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
# Option 4: sapply(split(DT$pwgtp15,DT$SEX),mean)
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
# Option 5: mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
# Option 6: tapply(DT$pwgtp15,DT$SEX,mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
fileUrlQ5 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrlQ5, destfile = "q5.csv", method = "curl")
library(data.table)
DT = fread('Q5.csv')
View(DT)
# Option 3: rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
# Option 3: rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
system.time(rowMeans(DT)[DT$SEX==1], rowMeans(DT)[DT$SEX==2])
DT$SEX <- as.numeric(DT$SEX)
# Option 3: rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
system.time(rowMeans(DT)[DT$SEX==1], rowMeans(DT)[DT$SEX==2])
# Option 3: rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
system.time(rowMeans(DT)[DT$SEX==1])
DT$SEX <- as.numeric(DT$SEX)
# Option 3: rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
system.time(rowMeans(DT)[DT$SEX==1])
system.time(rowMeans(DT)[DT$SEX==2])
system.time(mean(DT[DT$SEX==2,]$pwgtp15))
# Option 5: mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(mean(DT[DT$SEX==1,]$pwgtp15))
library(sqldf)
fileUrlQ2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrlQ2, destfile = "q1.csv", method = "curl")
acs <- read.csv("q2.csv")
download.file(fileUrlQ2, destfile = "q2.csv", method = "curl")
acs <- read.csv("q2.csv")
View(acs)
sqldf("select pwgtp1 from acs where AGEP < 50") # answer
answer_q2 <- sqldf("select pwgtp1 from acs where AGEP < 50") # answer
View(answer_q2)
answer_q2 <- sqldf("select pwgtp1, AGEP from acs where AGEP < 50") # answer
View(answer_q2)
## Q3
sqldf("select distinct AGEP from acs")
## Q3
sqldf("select distinct AGEP from acs order by AGEP")
sqldf("select unique * from acs")
## Q4
# How many characters are in the 10th, 20th, 30th and 100th lines of
# HTML from this page:
# url: http://biostat.jhsph.edu/~jleek/contact.html
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode = readLines(con)
close(con)
c(nchar(htmlCode[10]), nchar(htmlCode[20]), nchar(htmlCode[30]), nchar(htmlCode[100]))
## Q5
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
lines <- readLines(url, n = 10)
lines
download.file(url, destfile = "q5.csv", method = "curl")
df_q5 <- read.csv("q5.csv")
View(df_q5)
download.file(fileUrl, destfile = "q5.for", method = "curl")
download.file(url, destfile = "q5.for", method = "curl")
q5_df <- read.fwf(file = "q5.for", widths = c(15, 4, 1, 3, 5, 4),
header = FALSE, sep = "\t", skip = 4)
View(q5_df)
sum(q5_df$V6)
setwd("~/Data Science Projects/Coursera/JHU Data Science Specialization/Course 3 - Getting and Cleaning Data/Week4_Quiz")
fileUrlQ1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrlQ1, destfile = "q1.csv", method = "curl")
q1 <- read.csv("q1.csv")
View(q1)
names(q1)
?strsplit
strsplit(q1, "wgtp")
strsplit(names(q1), "wgtp")
strsplit(names(q1), "wgtp")
# Check names first of dataframe, get the value from position 123 afterwards
strsplit(names(q1), "wgtp")[123]
fileUrlQ2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fileUrlQ2, destfile = "q2.csv", method = "curl")
q2 <- read.csv("q2.csv")
View(q2)
fileUrlQ2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fileUrlQ2, destfile = "q2.csv", method = "curl")
q2 <- read.csv("q2.csv", skip = 4, nrows=190)
View(q2)
q2 <- q2[,c(1, 2, 4, 5)]
colnames(q2)[4] <- "GDP"
View(q2)
q2$GDP <- gsub(",", "", q2$GDP)
View(q2)
mean(q2$GDP)
fileUrlQ2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fileUrlQ2, destfile = "q2.csv", method = "curl")
q2 <- read.csv("q2.csv", skip = 4, nrows=190)
q2 <- q2[,c(1, 2, 4, 5)]
colnames(q2)[4] <- "GDP"
q2$GDP <- as.numeric(gsub(",", "", q2$GDP))
mean(q2$GDP)
View(q2)
colnames(q2)[3] <- "Country"
View(q2)
colnames(q2)[3] <- "countryNames"
grep("^United",countryNames)
grep("^United", q2$countryNames)
View(q2)
countryNames <- q2[3]
View(countryNames)
grep("^United", countryNames)
str(countryNames)
View(countryNames)
grep("^United",q2[, countryNames])
grep("^United",q2[,3])
fileUrlQ4_1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fileUrlQ3_1, destfile = "q4_1.csv", method = "curl")
q4_1 <- read.csv("q4_1.csv", skip = 4, nrows=190)
fileUrlQ4_2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(fileUrlQ4_2, destfile = "q4_2.csv", method = "curl")
q4_2 <- read.csv("q4_2.csv")
fileUrlQ4_1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fileUrlQ4_1, destfile = "q4_1.csv", method = "curl")
q4_1 <- read.csv("q4_1.csv", skip = 4, nrows=190)
library(dplyr)
q3_1 <- q3_1[,c(1, 2, 4, 5)]
colnames(q3_1)[1] <- "CountryCode"
colnames(q3_1)[2] <- "Rank"
colnames(q3_1)[3] <- "Country"
colnames(q3_1)[4] <- "GDP"
#q3_2 <- select(q3_2, CountryCode)
q3 <- merge(q3_1, q3_2, by="CountryCode")
library(dplyr)
q4_1 <- q4_1[,c(1, 2, 4, 5)]
colnames(q4_1)[1] <- "CountryCode"
colnames(q4_1)[2] <- "Rank"
colnames(q4_1)[3] <- "Country"
colnames(q4_1)[4] <- "GDP"
q4 <- merge(q4_1, q4_2, by="CountryCode")
View(q4)
View(q4)
View(q4)
grepl("Fiscal year end: June", q4$Special.Notes)
sum(grepl("Fiscal year end: June", q4$Special.Notes))
install.packages('quantmod')
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
View(amzn)
head(sampleTimes)
View(amzn)
# How many values were collected on Mondays in 2012?
install.packages('lubridate')
# How many values were collected on Mondays in 2012?
library(lubridate)
sum(weekdays(as.Date(sampleTimes[grep("^2012",sampleTimes)]))=="Monday")
# How many values were collected in 2012?
length(grep("^2012",sampleTimes))
